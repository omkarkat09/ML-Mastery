{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace89566-0751-4716-927d-ce2322420ff1",
   "metadata": {},
   "source": [
    " Data Collection (Stock Prices + News Sentiment Analysis)\n",
    " \n",
    "  1. Stock Price Data (Yahoo Finance API)\n",
    "  Weâ€™ll use yfinance to collect historical stock prices (Open, High, Low, Close, Volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7340b759-2ac7-4901-9843-435b7f11b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock data saved successfully!\n",
      "Price           Close       High        Low       Open    Volume\n",
      "Ticker            CMC        CMC        CMC        CMC       CMC\n",
      "Date                                                            \n",
      "2020-01-02  20.491957  20.812860  20.326921  20.812860   1901200\n",
      "2020-01-03  20.336090  20.620318  19.960175  19.969344   3358000\n",
      "2020-01-06  20.281078  22.041461  19.620934  21.839750  10063900\n",
      "2020-01-07  20.767017  20.950391  20.106874  20.161886   3669600\n",
      "2020-01-08  21.243788  21.463835  20.812861  20.858704   3258900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Choose a stock \n",
    "ticker = \"CMC\"  \n",
    "\n",
    "# Fetch historical stock data\n",
    "stock_data = yf.download(ticker, start=\"2020-01-01\", end=\"2024-01-01\")\n",
    "\n",
    "# Save as CSV\n",
    "stock_data.to_csv(\"C:/Users/dell/Documents/GitHub/ML-Mastery/Final_Projects/Stock_Price_Prediction/data/stock_prices.csv\")\n",
    "print(\"Stock data saved successfully!\")\n",
    "\n",
    "# Preview data\n",
    "print(stock_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6948a-d4e6-4629-ad0b-55ea553471b2",
   "metadata": {},
   "source": [
    " 2. Financial News Data (NewsAPI.org)\n",
    " News headlines affect stock prices! We'll fetch financial news and apply Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddd6b965-7671-4c41-8c90-446f2966eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " News data saved successfully!\n",
      "                   date                                           headline  \\\n",
      "0  2025-01-22T18:25:37Z  Jamie Dimon says he 'hugged it out' with Elon ...   \n",
      "1  2025-01-13T11:09:23Z  Top 25 global banks post 27% increase in marke...   \n",
      "2  2025-01-15T19:34:01Z  Jamie Dimon addresses a fresh round of CEO suc...   \n",
      "3  2025-01-23T21:36:45Z  JPMorgan boosts CEO Jamie Dimon's pay to $39 m...   \n",
      "4  2025-02-06T04:44:45Z  Jamie Dimon says he didn't run for president b...   \n",
      "\n",
      "                                         description  \n",
      "0  JPMorgan Chase CEO Jamie Dimon wished Elon Mus...  \n",
      "1  JPMorgan Chase continues to lead the way; Gold...  \n",
      "2  Jamie Dimon said there is a running list of ex...  \n",
      "3  Jamie Dimon was awarded an 8.3% pay raise foll...  \n",
      "4  \"It is subjecting your family to some very tou...  \n"
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize News API Client\n",
    "newsapi = NewsApiClient(api_key=\"e96684bf3929444a8ce78aacde55f703\")\n",
    "\n",
    "# Fetch top news related to our stock\n",
    "query = \"JPMorgan Chase\"  \n",
    "articles = newsapi.get_everything(q=query, from_param=\"2025-01-13\", sort_by=\"relevancy\", language=\"en\")\n",
    "\n",
    "# Extract useful fields\n",
    "news_data = []\n",
    "for article in articles[\"articles\"]:\n",
    "    news_data.append([article[\"publishedAt\"], article[\"title\"], article[\"description\"]])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_news = pd.DataFrame(news_data, columns=[\"date\", \"headline\", \"description\"])\n",
    "\n",
    "# Save as CSV\n",
    "df_news.to_csv(\"C:/Users/dell/Documents/GitHub/ML-Mastery/Final_Projects/Stock_Price_Prediction/data/stock_prices.csv\", index=False)\n",
    "print(\" News data saved successfully!\")\n",
    "\n",
    "# Preview data\n",
    "print(df_news.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be56052b-3a8b-46ed-9039-e940d9d48709",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['title', 'content', 'publishedAt'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_news\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_cleaned\u001b[38;5;241m.\u001b[39mhead())  \u001b[38;5;66;03m# Check the cleaned data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 23\u001b[0m, in \u001b[0;36mpreprocess_news\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_news\u001b[39m(df):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Extract relevant columns\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpublishedAt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Remove duplicates and NaNs\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     df\u001b[38;5;241m.\u001b[39mdrop_duplicates(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['title', 'content', 'publishedAt'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Handle cases where text might be NaN or None\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove links\n",
    "    text = re.sub(r\"\\W\", \" \", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_news(df):\n",
    "    # Extract relevant columns\n",
    "    df = df[[\"title\", \"description\", \"content\", \"publishedAt\"]].copy()\n",
    "    \n",
    "    # Remove duplicates and NaNs\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(subset=[\"title\", \"description\", \"content\"], inplace=True)\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    df[\"title\"] = df[\"title\"].apply(clean_text)\n",
    "    df[\"description\"] = df[\"description\"].apply(clean_text)\n",
    "    df[\"content\"] = df[\"content\"].apply(clean_text)\n",
    "    \n",
    "    # Combine text fields for sentiment analysis\n",
    "    df[\"combined_text\"] = df[\"title\"] + \" \" + df[\"description\"] + \" \" + df[\"content\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "df_cleaned = preprocess_news(df_news)\n",
    "print(df_cleaned.head())  # Check the cleaned data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
